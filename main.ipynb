{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train two different models. One cnn will use the written digit training data to predict the label, while the other, an rnn, will use the spoken audio data to do the same. To make our final predictions, we will use the model which exhibits the highest confidence for each guess. We will procede first by training the cnn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get labels, explore distribution\n",
    "training_labels = pd.read_csv(\"data/y_train.csv\")[\"label\"]\n",
    "training_labels.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataset wrapper\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_file, labels_file=None, is_test=False):\n",
    "        self.data = np.load(data_file)\n",
    "        if not is_test:\n",
    "            self.labels = pd.read_csv(labels_file)[\"label\"]\n",
    "        else:\n",
    "            self.labels = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            sample = {\n",
    "                'data': torch.tensor(self.data[idx].reshape(1, 28, 28), dtype=torch.float),\n",
    "                'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            sample = {\n",
    "                'data': torch.tensor(self.data[idx].reshape(1, 28, 28), dtype=torch.float)\n",
    "            }\n",
    "        return sample\n",
    "\n",
    "\n",
    "def get_data_loaders(data_file, labels_file, batch_size=64, validation_size=0.2):\n",
    "    dataset = CustomDataset(data_file, labels_file)\n",
    "    \n",
    "    #split dataset into training and validation sets\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(len(dataset)),\n",
    "        test_size=validation_size,\n",
    "        random_state=21,\n",
    "        stratify=dataset.labels  \n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataset = CustomDataset(\"data/x_train_wr.npy\", \"data/y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom cnn implementation\n",
    "#CNN implementation\n",
    "\n",
    "class cnn_block(nn.Module):\n",
    "  def __init__(self, in_channels = 3, n_hidden = 5, kernel_size = (2, 2)):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Conv2d(in_channels       = in_channels, out_channels = n_hidden, kernel_size = kernel_size, bias=False, padding = 'same'),\n",
    "        nn.BatchNorm2d(num_features = n_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Conv2d(in_channels       = n_hidden, out_channels = in_channels, kernel_size = kernel_size, bias=False, padding = 'same'),\n",
    "        nn.BatchNorm2d(num_features = in_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2))\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.layers(x)\n",
    "\n",
    "\n",
    "class linear_block(nn.Module):\n",
    "  def __init__(self, in_features, n_hidden):\n",
    "    super().__init__()\n",
    "    self.in_features = (in_features, n_hidden)\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Linear(in_features = in_features, out_features = n_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(in_features = n_hidden, out_features = in_features),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.layers(x)\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "  def __init__(self, in_channels = 1, cnn_channels = 1, linear_hidden = 500, n_classes = 10, kernel_size = (3, 3)):\n",
    "    super().__init__()\n",
    "\n",
    "    \n",
    "    self.cnn_layers = nn.Sequential(\n",
    "        cnn_block(in_channels, cnn_channels, kernel_size),\n",
    "        cnn_block(in_channels, cnn_channels, kernel_size),\n",
    "        cnn_block(in_channels, cnn_channels, kernel_size))\n",
    "\n",
    "\n",
    "    self.down_sample = nn.Conv2d(in_channels = in_channels, out_channels = 1, kernel_size = (1, 1))\n",
    "\n",
    "    self.linear_layers = nn.Sequential(\n",
    "        linear_block(28*28, linear_hidden),\n",
    "        linear_block(28*28, linear_hidden)\n",
    "    )\n",
    "    self.last_layer = nn.Linear(28*28, n_classes)\n",
    "\n",
    "    self.all        = nn.Sequential(\n",
    "        self.cnn_layers,\n",
    "        self.down_sample,\n",
    "        nn.Flatten(),\n",
    "        self.linear_layers,\n",
    "        self.last_layer,\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.all(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train cnn\n",
    "\n",
    "#training \n",
    "def train_cnn(model, train_loader, val_loader, optimizer, criterion, epochs=5, device='cpu'):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/ {epochs}\", unit=\"batch\"):\n",
    "            inputs, labels = batch['data'].to(device), batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, labels = batch['data'].to(device), batch['label'].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch['data'].to(device), batch['label'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and validation sets with 80-20 split\n",
    "batch_size = 64\n",
    "validation_size = 0.2\n",
    "train_loader, val_loader = get_data_loaders(\"data/x_train_wr.npy\", \"data/y_train.csv\", batch_size=batch_size, validation_size=validation_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next portion of the code tries out different hyperparams by training different models on a small subset of the training data (1/5 of the training data) for only 3 epochs. It takes a bit to run, but after expirementing, {'cnn_channels': 3, 'linear_hidden': 25, 'kernel_size': (5, 5)} TENDS to work the best (although other hyperparams are close) with an F1 score of roughly .94 (which is pretty good considering it's only training on a small subset of the total data). If you want to check my hyperparam comparision, you can simply change the variable below to True to run this process. Otherwise, we simply default to the previously described hyperparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_a_lot_of_time = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a smaller dataset for hyperparam comparisons \n",
    "hyper_loader, hyper_val_loader = get_data_loaders(\"data/x_train_wr.npy\", \"data/y_train.csv\", batch_size=batch_size, validation_size=.8) #thus we train on .2 of given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: cnn_channels=2, linear_hidden=25, kernel_size=(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 3:   0%|          | 0/188 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 3: 100%|██████████| 188/188 [00:05<00:00, 31.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 1.8648, Train Accuracy: 0.3579, Validation Loss: 0.7279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/ 3: 100%|██████████| 188/188 [00:05<00:00, 35.44batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train Loss: 0.5623, Train Accuracy: 0.8304, Validation Loss: 0.4149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/ 3: 100%|██████████| 188/188 [00:05<00:00, 33.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train Loss: 0.4300, Train Accuracy: 0.8667, Validation Loss: 0.3484\n",
      "Validation F1 for cnn_channels=2, linear_hidden=25, kernel_size=(3, 3): 0.8915425775747148\n",
      "Training model: cnn_channels=2, linear_hidden=25, kernel_size=(5, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 3: 100%|██████████| 188/188 [00:06<00:00, 30.36batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 1.1627, Train Accuracy: 0.6142, Validation Loss: 0.3479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/ 3: 100%|██████████| 188/188 [00:05<00:00, 31.84batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train Loss: 0.3299, Train Accuracy: 0.8971, Validation Loss: 0.2352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/ 3: 100%|██████████| 188/188 [00:05<00:00, 32.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train Loss: 0.2512, Train Accuracy: 0.9197, Validation Loss: 0.1878\n",
      "Validation F1 for cnn_channels=2, linear_hidden=25, kernel_size=(5, 5): 0.9445698893922014\n",
      "Training model: cnn_channels=2, linear_hidden=50, kernel_size=(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 3: 100%|██████████| 188/188 [00:05<00:00, 33.19batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.7376, Train Accuracy: 0.7602, Validation Loss: 0.3659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/ 3: 100%|██████████| 188/188 [00:05<00:00, 36.15batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train Loss: 0.3812, Train Accuracy: 0.8813, Validation Loss: 0.2964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/ 3: 100%|██████████| 188/188 [00:04<00:00, 38.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train Loss: 0.3114, Train Accuracy: 0.9028, Validation Loss: 0.2367\n",
      "Validation F1 for cnn_channels=2, linear_hidden=50, kernel_size=(3, 3): 0.9298038858371621\n",
      "Training model: cnn_channels=2, linear_hidden=50, kernel_size=(5, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 3: 100%|██████████| 188/188 [00:05<00:00, 31.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.6928, Train Accuracy: 0.7765, Validation Loss: 0.3062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/ 3: 100%|██████████| 188/188 [00:05<00:00, 31.76batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train Loss: 0.3165, Train Accuracy: 0.9007, Validation Loss: 0.2350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/ 3: 100%|██████████| 188/188 [00:06<00:00, 28.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train Loss: 0.2501, Train Accuracy: 0.9213, Validation Loss: 0.1718\n",
      "Validation F1 for cnn_channels=2, linear_hidden=50, kernel_size=(5, 5): 0.9464691864656188\n",
      "Training model: cnn_channels=3, linear_hidden=25, kernel_size=(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 3: 100%|██████████| 188/188 [00:05<00:00, 36.32batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 1.3285, Train Accuracy: 0.5817, Validation Loss: 0.5136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/ 3: 100%|██████████| 188/188 [00:05<00:00, 36.43batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train Loss: 0.4922, Train Accuracy: 0.8548, Validation Loss: 0.3992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/ 3: 100%|██████████| 188/188 [00:05<00:00, 34.76batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train Loss: 0.3993, Train Accuracy: 0.8802, Validation Loss: 0.3432\n",
      "Validation F1 for cnn_channels=3, linear_hidden=25, kernel_size=(3, 3): 0.8925440790068382\n",
      "Training model: cnn_channels=3, linear_hidden=25, kernel_size=(5, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 3: 100%|██████████| 188/188 [00:06<00:00, 28.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.6239, Train Accuracy: 0.8040, Validation Loss: 0.2612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/ 3: 100%|██████████| 188/188 [00:06<00:00, 27.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train Loss: 0.2791, Train Accuracy: 0.9155, Validation Loss: 0.1944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/ 3: 100%|██████████| 188/188 [00:06<00:00, 29.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train Loss: 0.2170, Train Accuracy: 0.9349, Validation Loss: 0.1904\n",
      "Validation F1 for cnn_channels=3, linear_hidden=25, kernel_size=(5, 5): 0.9416727401726026\n",
      "Training model: cnn_channels=3, linear_hidden=50, kernel_size=(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 3: 100%|██████████| 188/188 [00:05<00:00, 34.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.9529, Train Accuracy: 0.6994, Validation Loss: 0.3799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/ 3: 100%|██████████| 188/188 [00:05<00:00, 34.76batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train Loss: 0.4083, Train Accuracy: 0.8755, Validation Loss: 0.3211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/ 3: 100%|██████████| 188/188 [00:05<00:00, 36.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train Loss: 0.3236, Train Accuracy: 0.9033, Validation Loss: 0.2616\n",
      "Validation F1 for cnn_channels=3, linear_hidden=50, kernel_size=(3, 3): 0.9201871468026251\n",
      "Training model: cnn_channels=3, linear_hidden=50, kernel_size=(5, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 3: 100%|██████████| 188/188 [00:06<00:00, 28.52batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.6594, Train Accuracy: 0.7903, Validation Loss: 0.2989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/ 3: 100%|██████████| 188/188 [00:06<00:00, 29.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train Loss: 0.3095, Train Accuracy: 0.9049, Validation Loss: 0.2299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/ 3: 100%|██████████| 188/188 [00:07<00:00, 26.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train Loss: 0.2518, Train Accuracy: 0.9220, Validation Loss: 0.2239\n",
      "Validation F1 for cnn_channels=3, linear_hidden=50, kernel_size=(5, 5): 0.9306594066717102\n",
      "Best Hyperparameters: {'cnn_channels': 2, 'linear_hidden': 50, 'kernel_size': (5, 5)}\n",
      "Best Validation Accuracy: 0.9464691864656188\n"
     ]
    }
   ],
   "source": [
    "#test different hyperparams \n",
    "\n",
    "hyperparameters = {\n",
    "    'cnn_channels': [2, 3],  # Vary the number of channels in the CNN layers\n",
    "    'linear_hidden': [25, 50],  # Vary the number of hidden units in linear layers\n",
    "    'kernel_size': [(3, 3), (5, 5)]  # Vary the kernel size of the convolutional layers\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "best_f1 = 0.0\n",
    "best_hyperparameters = {}\n",
    "if (waste_a_lot_of_time):\n",
    "    for cnn_channels in hyperparameters['cnn_channels']:\n",
    "        for linear_hidden in hyperparameters['linear_hidden']:\n",
    "            for kernel_size in hyperparameters['kernel_size']:\n",
    "                print(f\"Training model: cnn_channels={cnn_channels}, linear_hidden={linear_hidden}, kernel_size={kernel_size}\")\n",
    "                model = CNNClassifier(cnn_channels=cnn_channels, linear_hidden=linear_hidden, kernel_size=kernel_size)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                train_cnn(model, hyper_loader, hyper_val_loader, optimizer, criterion, epochs=3)\n",
    "                val_accuracy = validate(model, hyper_val_loader)\n",
    "                print(f\"Validation F1 for cnn_channels={cnn_channels}, linear_hidden={linear_hidden}, kernel_size={kernel_size}: {val_accuracy}\")\n",
    "                if val_accuracy > best_f1:\n",
    "                    best_f1 = val_accuracy\n",
    "                    best_hyperparameters = {'cnn_channels': cnn_channels, 'linear_hidden': linear_hidden, 'kernel_size': kernel_size}\n",
    "else: \n",
    "    best_hyperparameters = {'cnn_channels': 3, 'linear_hidden': 25, 'kernel_size': (5, 5)}\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best Validation Accuracy:\", best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 5:   0%|          | 0/750 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 5: 100%|██████████| 750/750 [00:26<00:00, 28.01batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.3839, Train Accuracy: 0.8742, Validation Loss: 0.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/ 5: 100%|██████████| 750/750 [00:28<00:00, 26.65batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Train Loss: 0.1834, Train Accuracy: 0.9421, Validation Loss: 0.1343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/ 5: 100%|██████████| 750/750 [00:28<00:00, 26.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Train Loss: 0.1481, Train Accuracy: 0.9534, Validation Loss: 0.1068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/ 5: 100%|██████████| 750/750 [00:25<00:00, 29.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Train Loss: 0.1237, Train Accuracy: 0.9604, Validation Loss: 0.0995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/ 5: 100%|██████████| 750/750 [00:24<00:00, 30.21batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 0.1095, Train Accuracy: 0.9653, Validation Loss: 0.0896\n"
     ]
    }
   ],
   "source": [
    "#train model with selected hyperparams\n",
    "best_model = CNNClassifier(cnn_channels=best_hyperparameters['cnn_channels'], linear_hidden=best_hyperparameters['linear_hidden'], kernel_size=best_hyperparameters['kernel_size'])\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_cnn(best_model, train_loader, val_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_test_loader(data_file, batch_size=64):\n",
    "    test_dataset = CustomDataset(data_file, is_test=True)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False \n",
    "    )\n",
    "    return test_loader\n",
    "\n",
    "test_data_loader = get_test_loader(\"data/x_test_wr.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted labels\n",
    "\n",
    "best_model.eval()  # set the mode to eval\n",
    "predicted_labels = []\n",
    "confidence_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader:\n",
    "        inputs = batch['data'].to(device)  \n",
    "        outputs = best_model(inputs)\n",
    "        probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        confidence_scores.extend(probabilities.gather(1, predicted.view(-1, 1)).squeeze().cpu().numpy()) #only add the highest confidence score, or the score of the predicted label\n",
    "\n",
    "# Convert predictions to numpy array\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "confidence_scores = np.array(confidence_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cnn_model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 507)\n"
     ]
    }
   ],
   "source": [
    "#now we will make a model that predicts the label based on the spoken audio\n",
    "\n",
    "#get the data\n",
    "audio_training_data = np.load(\"data/x_train_sp.npy\")\n",
    "print(audio_training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our model, we will use a simple RNN-style model to predict labels based on our audio\n",
    "\n",
    "\n",
    "class RNNBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0):\n",
    "        super(RNNBlock, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, input_size)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, linear_hidden, num_classes):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.rnn_layers = RNNBlock(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            LinearBlock(hidden_size, linear_hidden),  # Input size is now hidden_size\n",
    "            LinearBlock(linear_hidden, linear_hidden)\n",
    "        )\n",
    "        self.output_layer = nn.Linear(linear_hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.rnn_layers(x)\n",
    "        out = torch.mean(out, dim=1)  # Global average pooling over the sequence dimension\n",
    "        out = self.linear_layers(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 520)\n"
     ]
    }
   ],
   "source": [
    "print(\"Old shape:\" + audio_training_data.shape)\n",
    "\n",
    "#change the length of each sample from 507 to 520 (a more convenient size for our model) by padding each sample with 0s at the end\n",
    "audio_training_data = np.pad(audio_training_data, ((0, 0), (0, 13)), mode='constant') \n",
    "\n",
    "#convert into tensors\n",
    "audio_training_data_tens = torch.from_numpy(audio_training_data).float()\n",
    "training_labels_tens = torch.from_numpy(np.array(training_labels)).long()\n",
    "\n",
    "print(\"New shape: \" + audio_training_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x32 and 128x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, target_labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 43\u001b[0m, in \u001b[0;36mRNNClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn_layers(x)\n\u001b[0;32m     42\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Global average pooling over the sequence dimension\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(out)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 24\u001b[0m, in \u001b[0;36mLinearBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 24\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m     26\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x32 and 128x256)"
     ]
    }
   ],
   "source": [
    "rnn_model = RNNClassifier(520, 128, 2, 256, 10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=.01, momentum=.9)\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, len(audio_training_data_tens), batch_size):\n",
    "        # Get mini-batch\n",
    "        inputs = audio_training_data_tens[i:i+batch_size]\n",
    "        target_labels = training_labels_tens[i:i+batch_size]\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = rnn_model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, target_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save predictions\n",
    "predictions_df = pd.DataFrame({'row_id': np.array([i for i in range(len(predicted_labels))]), 'label': predicted_labels.flatten()})\n",
    "predictions_df.to_csv('Caleb Elizondo preds.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
