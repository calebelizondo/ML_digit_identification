{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train two different models. One cnn will use the written digit training data to predict the label, while the other, an rnn, will use the spoken audio data to do the same. To make our final predictions, we will use the model which exhibits the highest confidence for each guess. We will procede first by training the cnn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import deps\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataset wrapper\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_file, labels_file=None, is_test=False, RNN=False):\n",
    "        self.RNN = RNN\n",
    "        self.data = np.load(data_file)\n",
    "        if not is_test:\n",
    "            self.labels = pd.read_csv(labels_file)[\"label\"]\n",
    "        else:\n",
    "            self.labels = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            if not self.RNN: \n",
    "                sample = {\n",
    "                    'data': torch.tensor(self.data[idx].reshape(1, 28, 28), dtype=torch.float),\n",
    "                    'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "                }\n",
    "            else: \n",
    "                sample = {\n",
    "                    'data': torch.tensor(self.data[idx], dtype=torch.float),\n",
    "                    'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "                }\n",
    "\n",
    "        else:\n",
    "            if not self.RNN: \n",
    "                sample = {\n",
    "                    'data': torch.tensor(self.data[idx].reshape(1, 28, 28), dtype=torch.float)\n",
    "                }\n",
    "            else: \n",
    "                sample = {\n",
    "                    'data': torch.tensor(self.data[idx], dtype=torch.float),\n",
    "                }\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "# helper function to retrieve and format data into loaders\n",
    "def get_data_loaders(data_file, labels_file, batch_size=64, validation_size=0.2, RNN=False):\n",
    "    dataset = CustomDataset(data_file, labels_file, RNN=RNN)\n",
    "    \n",
    "    #split dataset into training and validation sets\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(len(dataset)),\n",
    "        test_size=validation_size,\n",
    "        random_state=21,\n",
    "        stratify=dataset.labels  \n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data\n",
    "img_dataset = CustomDataset(\"data/x_train_wr.npy\", \"data/y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom cnn implementation\n",
    "class cnn_block(nn.Module):\n",
    "  def __init__(self, in_channels = 3, n_hidden = 5, kernel_size = (2, 2)):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Conv2d(in_channels       = in_channels, out_channels = n_hidden, kernel_size = kernel_size, bias=False, padding = 'same'),\n",
    "        nn.BatchNorm2d(num_features = n_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Conv2d(in_channels       = n_hidden, out_channels = in_channels, kernel_size = kernel_size, bias=False, padding = 'same'),\n",
    "        nn.BatchNorm2d(num_features = in_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2))\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.layers(x)\n",
    "\n",
    "\n",
    "class linear_block(nn.Module):\n",
    "  def __init__(self, in_features, n_hidden):\n",
    "    super().__init__()\n",
    "    self.in_features = (in_features, n_hidden)\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Linear(in_features = in_features, out_features = n_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(in_features = n_hidden, out_features = in_features),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.layers(x)\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "  def __init__(self, in_channels = 1, cnn_channels = 1, linear_hidden = 500, n_classes = 10, kernel_size = (3, 3)):\n",
    "    super().__init__()\n",
    "\n",
    "    \n",
    "    self.cnn_layers = nn.Sequential(\n",
    "        cnn_block(in_channels, cnn_channels, kernel_size),\n",
    "        cnn_block(in_channels, cnn_channels, kernel_size),\n",
    "        cnn_block(in_channels, cnn_channels, kernel_size))\n",
    "\n",
    "\n",
    "    self.down_sample = nn.Conv2d(in_channels = in_channels, out_channels = 1, kernel_size = (1, 1))\n",
    "\n",
    "    self.linear_layers = nn.Sequential(\n",
    "        linear_block(28*28, linear_hidden),\n",
    "        linear_block(28*28, linear_hidden)\n",
    "    )\n",
    "    self.last_layer = nn.Linear(28*28, n_classes)\n",
    "\n",
    "    self.all        = nn.Sequential(\n",
    "        self.cnn_layers,\n",
    "        self.down_sample,\n",
    "        nn.Flatten(),\n",
    "        self.linear_layers,\n",
    "        self.last_layer,\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.all(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to train cnn\n",
    "def train_cnn(model, train_loader, val_loader, optimizer, criterion, epochs=5, device='cpu'):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/ {epochs}\", unit=\"batch\"):\n",
    "            inputs, labels = batch['data'].to(device), batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, labels = batch['data'].to(device), batch['label'].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#measures f1 score of model\n",
    "def validate(model, val_loader, RNN=False):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch['data'].to(device), batch['label'].to(device)\n",
    "            hidden = 0\n",
    "            outputs = 0\n",
    "            if RNN: \n",
    "                hidden = model.initHidden().to(device)\n",
    "                outputs = model(inputs, hidden)\n",
    "            else: \n",
    "                outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and validation sets with 80-20 split\n",
    "batch_size = 64\n",
    "validation_size = 0.2\n",
    "train_loader, val_loader = get_data_loaders(\"data/x_train_wr.npy\", \"data/y_train.csv\", batch_size=batch_size, validation_size=validation_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next portion of the code tries out different hyperparams by training different models on a small subset of the training data (1/5 of the training data) for only 3 epochs. It takes a bit to run, but after expirementing, {'cnn_channels': 3, 'linear_hidden': 25, 'kernel_size': (5, 5)} TENDS to work the best (although other hyperparams are close) with an F1 score of roughly .94 (which is pretty good considering it's only training on a small subset of the total data). If you want to check my hyperparam comparision, you can simply change the variable below to True to run this process. Otherwise, we simply default to the previously described hyperparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_a_lot_of_time = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a smaller dataset for hyperparam comparisons \n",
    "hyper_loader, hyper_val_loader = get_data_loaders(\"data/x_train_wr.npy\", \"data/y_train.csv\", batch_size=batch_size, validation_size=.8) #thus we train on .2 of given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'cnn_channels': 3, 'linear_hidden': 25, 'kernel_size': (5, 5)}\n",
      "Best Validation Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "#test different hyperparams \n",
    "\n",
    "hyperparameters = {\n",
    "    'cnn_channels': [2, 3],  # Vary the number of channels in the CNN layers\n",
    "    'linear_hidden': [25, 50],  # Vary the number of hidden units in linear layers\n",
    "    'kernel_size': [(3, 3), (5, 5)]  # Vary the kernel size of the convolutional layers\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "best_f1 = 0.0\n",
    "best_hyperparameters = {}\n",
    "if (waste_a_lot_of_time):\n",
    "    for cnn_channels in hyperparameters['cnn_channels']:\n",
    "        for linear_hidden in hyperparameters['linear_hidden']:\n",
    "            for kernel_size in hyperparameters['kernel_size']:\n",
    "                print(f\"Training model: cnn_channels={cnn_channels}, linear_hidden={linear_hidden}, kernel_size={kernel_size}\")\n",
    "                model = CNNClassifier(cnn_channels=cnn_channels, linear_hidden=linear_hidden, kernel_size=kernel_size)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                train_cnn(model, hyper_loader, hyper_val_loader, optimizer, criterion, epochs=3)\n",
    "                val_accuracy = validate(model, hyper_val_loader)\n",
    "                print(f\"Validation F1 for cnn_channels={cnn_channels}, linear_hidden={linear_hidden}, kernel_size={kernel_size}: {val_accuracy}\")\n",
    "                if val_accuracy > best_f1:\n",
    "                    best_f1 = val_accuracy\n",
    "                    best_hyperparameters = {'cnn_channels': cnn_channels, 'linear_hidden': linear_hidden, 'kernel_size': kernel_size}\n",
    "else: \n",
    "    best_hyperparameters = {'cnn_channels': 3, 'linear_hidden': 25, 'kernel_size': (5, 5)}\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best Validation F1:\", best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 5: 100%|██████████| 750/750 [00:26<00:00, 28.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.3579, Train Accuracy: 0.8863, Validation Loss: 0.1557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/ 5: 100%|██████████| 750/750 [00:37<00:00, 20.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Train Loss: 0.1774, Train Accuracy: 0.9444, Validation Loss: 0.1285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/ 5: 100%|██████████| 750/750 [00:46<00:00, 16.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Train Loss: 0.1461, Train Accuracy: 0.9545, Validation Loss: 0.1156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/ 5: 100%|██████████| 750/750 [00:37<00:00, 19.87batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Train Loss: 0.1284, Train Accuracy: 0.9594, Validation Loss: 0.1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/ 5: 100%|██████████| 750/750 [00:33<00:00, 22.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 0.1180, Train Accuracy: 0.9614, Validation Loss: 0.0996\n"
     ]
    }
   ],
   "source": [
    "#train model with selected hyperparams\n",
    "best_model = CNNClassifier(cnn_channels=best_hyperparameters['cnn_channels'], linear_hidden=best_hyperparameters['linear_hidden'], kernel_size=best_hyperparameters['kernel_size'])\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_cnn(best_model, train_loader, val_loader, optimizer, criterion, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to get the test data\n",
    "\n",
    "def get_test_loader(data_file, batch_size=64, RNN=False):\n",
    "    test_dataset = CustomDataset(data_file, is_test=True, RNN=RNN)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False \n",
    "    )\n",
    "    return test_loader\n",
    "\n",
    "test_data_loader = get_test_loader(\"data/x_test_wr.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted labels on the test data\n",
    "\n",
    "best_model.eval()  # set the mode to eval\n",
    "predicted_labels = []\n",
    "confidence_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader:\n",
    "        inputs = batch['data'].to(device)  \n",
    "        outputs = best_model(inputs)\n",
    "        probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        confidence_scores.extend(probabilities.gather(1, predicted.view(-1, 1)).squeeze().cpu().numpy()) #only add the highest confidence score, or the score of the predicted label\n",
    "\n",
    "# Convert predictions to numpy array\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "confidence_scores = np.array(confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_f1 = validate(best_model, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will turn to the audio data, for which we will classify using a custom RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom wrapper to format audio data\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_file, labels_file=None, is_test=False):\n",
    "        self.data = np.load(data_file)\n",
    "        if not is_test:\n",
    "            self.labels = pd.read_csv(labels_file)[\"label\"]\n",
    "        else:\n",
    "            self.labels = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        sample = {'data': torch.tensor(self.data[idx], dtype=torch.float).unsqueeze(0)}\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            sample['label'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train rnn\n",
    "def train_rnn(model, train_loader, val_loader, optimizer, criterion, epochs=8, device='cpu'):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/ {epochs}\", unit=\"batch\"):\n",
    "            inputs, labels = batch['data'].to(device), batch['label'].to(device)\n",
    "\n",
    "            hidden = model.initHidden().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, hidden)\n",
    "            labels = labels.long()\n",
    "            loss = criterion(outputs, labels) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, labels = batch['data'].to(device), batch['label'].to(device)\n",
    "                hidden = model.initHidden().to(device)\n",
    "                outputs = model(inputs, hidden)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve and format the audio data into dataloaders\n",
    "\n",
    "dataset = AudioDataset(\"data/x_train_sp.npy\", \"data/y_train.csv\")\n",
    "    \n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "        np.arange(len(dataset)),\n",
    "        test_size=validation_size,\n",
    "        random_state=21,\n",
    "        stratify=dataset.labels  \n",
    "    )\n",
    "    \n",
    "train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "    )\n",
    "    \n",
    "val_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom RNN implementation\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.middle_layer1 = nn.Linear(hidden_size, hidden_size)  # New middle layer\n",
    "        self.middle_layer2 = nn.Linear(hidden_size, hidden_size)  # Another new middle layer\n",
    "        self.middle_layer3 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden): \n",
    "        hidden = F.tanh(self.input_layer(input) + self.middle_layer1(hidden))  # Pass through first middle layer\n",
    "        hidden = F.tanh(self.middle_layer2(hidden))  # Pass through second middle layer\n",
    "        hidden = F.tanh(self.middle_layer3(hidden))\n",
    "        output = self.output_layer(hidden)\n",
    "        output = F.log_softmax(output.squeeze(1), dim=1)\n",
    "        return output\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform hyperparam tuning. This is done by training several models using different combinations of hyperparams on a small subsection of the data. Like the last time, it takes quite a while. By default, this process will not run unless the variable below is changed to True. Otherwise, we will default to {'learning_rate': 0.0001, 'linear_hidden': 600}, which tends to perform the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_a_lot_of_time = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data for hyperparam tuning. By setting the test_size param to .8, we will only use .2 percent of the data on hyperparam tuning\n",
    "\n",
    "dataset = AudioDataset(\"data/x_train_sp.npy\", \"data/y_train.csv\")\n",
    "    \n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "        np.arange(len(dataset)),\n",
    "        test_size=.8,\n",
    "        random_state=21,\n",
    "        stratify=dataset.labels  \n",
    "    )\n",
    "    \n",
    "hyper_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "    )\n",
    "    \n",
    "hyper_val_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.001, 'linear_hidden': 500}\n",
      "Best Validation Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Perform hyperparameter tuning\n",
    "\n",
    "hyperparameters = {\n",
    "    'learning_rate': [.001, .005, .01],\n",
    "    'linear_hidden': [400, 500, 600],  \n",
    "}\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_hyperparameters = {}\n",
    "if (waste_a_lot_of_time):\n",
    "    for learning_rate in hyperparameters['learning_rate']:\n",
    "        for linear_hidden in hyperparameters['linear_hidden']:\n",
    "            print(f\"Training model: hidden_size={linear_hidden}, learning_rate={learning_rate}\")\n",
    "            model = RNN(input_size=507, hidden_size=linear_hidden, output_size=10)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            train_rnn(model, hyper_loader, hyper_val_loader, optimizer, criterion, epochs=3)\n",
    "            val_accuracy = validate(model, hyper_val_loader, RNN=True)\n",
    "            print(f\"Validation F1 for hidden_size={linear_hidden}, learning_rate={learning_rate}: {val_accuracy}\")\n",
    "            if val_accuracy > best_f1:\n",
    "                best_f1 = val_accuracy\n",
    "                best_hyperparameters = {'learning_rate': learning_rate, 'linear_hidden': linear_hidden}\n",
    "else: \n",
    "    best_hyperparameters =  {'learning_rate': .001, 'linear_hidden': 500}\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best Validation F1:\", best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 14:   0%|          | 0/750 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 14: 100%|██████████| 750/750 [00:08<00:00, 86.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14, Train Loss: 1.7184, Train Accuracy: 0.3945, Validation Loss: 1.6881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/ 14: 100%|██████████| 750/750 [00:08<00:00, 93.55batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/14, Train Loss: 1.4643, Train Accuracy: 0.5009, Validation Loss: 1.3589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/ 14: 100%|██████████| 750/750 [00:08<00:00, 91.05batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/14, Train Loss: 1.3191, Train Accuracy: 0.5495, Validation Loss: 1.2583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/ 14: 100%|██████████| 750/750 [00:08<00:00, 90.86batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/14, Train Loss: 1.2050, Train Accuracy: 0.5854, Validation Loss: 1.2295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/ 14: 100%|██████████| 750/750 [00:08<00:00, 91.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/14, Train Loss: 1.0391, Train Accuracy: 0.6435, Validation Loss: 1.0460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/ 14: 100%|██████████| 750/750 [00:08<00:00, 89.31batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/14, Train Loss: 0.9219, Train Accuracy: 0.6828, Validation Loss: 0.9091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/ 14: 100%|██████████| 750/750 [00:08<00:00, 90.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/14, Train Loss: 0.8456, Train Accuracy: 0.7119, Validation Loss: 0.8387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/ 14: 100%|██████████| 750/750 [00:08<00:00, 91.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/14, Train Loss: 0.7778, Train Accuracy: 0.7315, Validation Loss: 0.6781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/ 14: 100%|██████████| 750/750 [00:09<00:00, 79.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/14, Train Loss: 0.7397, Train Accuracy: 0.7473, Validation Loss: 0.8252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/ 14: 100%|██████████| 750/750 [00:08<00:00, 85.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/14, Train Loss: 0.7010, Train Accuracy: 0.7599, Validation Loss: 0.6888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/ 14: 100%|██████████| 750/750 [00:09<00:00, 80.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/14, Train Loss: 0.6511, Train Accuracy: 0.7757, Validation Loss: 0.6187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/ 14: 100%|██████████| 750/750 [00:08<00:00, 86.21batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/14, Train Loss: 0.6236, Train Accuracy: 0.7855, Validation Loss: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/ 14: 100%|██████████| 750/750 [00:08<00:00, 87.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/14, Train Loss: 0.5941, Train Accuracy: 0.7961, Validation Loss: 0.6093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/ 14: 100%|██████████| 750/750 [00:08<00:00, 85.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/14, Train Loss: 0.5712, Train Accuracy: 0.8041, Validation Loss: 0.6711\n"
     ]
    }
   ],
   "source": [
    "#train the final rnn model on the chosen hyperparams \n",
    "model = RNN(507, best_hyperparameters[\"linear_hidden\"], 10)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_hyperparameters[\"learning_rate\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_rnn(model, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, criterion=criterion, epochs=14) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_f1 = validate(model, val_loader=val_loader, RNN=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the model to get our prediction and confidence levels for each sample in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get and format the test data\n",
    "test_dataset = AudioDataset(\"data/x_test_sp.npy\", is_test=True)\n",
    "\n",
    "test_data_loader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the predictions and confidences\n",
    "model.eval()  # set the mode to eval\n",
    "rnn_predicted_labels = []\n",
    "rnn_confidence_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader:\n",
    "        inputs = batch['data'].to(device)  \n",
    "        hidden = model.initHidden().to(device)\n",
    "        outputs = model(inputs, hidden)\n",
    "        probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "        max_probabilities, predicted = torch.max(probabilities, 1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        rnn_predicted_labels.extend(predicted.cpu().numpy())\n",
    "        rnn_confidence_scores.extend(max_probabilities.cpu().numpy()) \n",
    "        \n",
    "\n",
    "# Convert predictions to numpy array\n",
    "rnn_predicted_labels = np.array(predicted_labels)\n",
    "rnn_confidence_scores = np.array(confidence_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make final predictions by using the model which produces the highest confidence score\n",
    "final_predictions  = np.zeros(len(predicted_labels))\n",
    "\n",
    "#will use a weighted average for selection, as I don't want to treat the confidence levels the same since the CNN reliably outperforms the RNN most of the time\n",
    "for i in range(len(predicted_labels)): \n",
    "    if (rnn_f1 * rnn_confidence_scores[i]) > (cnn_f1 * confidence_scores[i]):\n",
    "        final_predictions[i] = int(rnn_predicted_labels[i])\n",
    "    else: \n",
    "        final_predictions[i] = int(predicted_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final predictions\n",
    "predictions_df = pd.DataFrame({'row_id': np.array([i for i in range(len(predicted_labels))]), 'label': final_predictions.astype(int).flatten()})\n",
    "predictions_df.to_csv('Caleb Elizondo preds.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
